# Data_Warehouse
project 3 - Data Engineer

## Table of contents
* [Introduction](#introduction)
* [Project Description](#project_description)
* [Datasets](#datasets)
* [Environment(s)](#environment)
* [Files](#Files)
* [Tables](#tables)
* [Process](#Process)

## Introduction
This project was part of my Udacity Training Degree in Data Engineering. A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

I was tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. The task was to test the ETL pipeline by running queries given to me by the analytics team from Sparkify.

## Project Description
In this project, I applied what I learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. To complete the project, I needed to load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

## Datasets
The datasets consisted of two sets residing in S3:
- **song data**: The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
- **Log Data**: The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

## Environments
For this project we used Jupyter Lab and worked with python files as well as cfg files.

## Files
- **create_tables.py** :  where fact and dimension tables for the star schema in Redshift were created.
- **etl.py** :  file where I loaded data from S3 into staging tables on Redshift and then process that data into my analytics tables on Redshift.
- **sql_queries.py** : where I defined SQL statements, which will be imported into the two other files above.
- **readme.md** : provide discussion on the process

## Tables
As the aim for this project was to create a ETL pipeline for the company to be able to query and analyze songs and user profiles, the schema selected is a star schema. The fact table contains the specific measurables or primary data to be analyzed and it was taken from the log data files:
**Fact Table**
- **songplays** - records in event data associated with song plays i.e. records with page NextSong:

songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
- **Dimension Tables**

**users - users in the app**

user_id, first_name, last_name, gender, level

**songs - songs in music database**

song_id, title, artist_id, year, duration

**artists - artists in music database**

artist_id, name, location, lattitude, longitude

**time - timestamps of records in songplays broken down into specific units**

start_time, hour, day, week, month, year, weekday


## Process
In order to complete the project, I needed to build and ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. The task was to test the ETL pipeline by running queries given to me by the analytics team from Sparkify. In order to save time in loading the data i did not load the full datasets and decided to go with a sample set for which I ran queries on via the query editor in Redshift in the cluster I had created there (with an associated IAM role, user and Region).

**Example of Queries**
1. List of top 10 users, their first and last name and info on if they are subscribers or listening via free service:

SELECT first_name, last_name, level

FROM user_table

GROUP BY first_name, last_name, level

ORDER BY COUNT(*) Desc

LIMIT    10;   

2. List of top 10 most listened songs from my sample:

SELECT sp.song_id, a.name, s.title

FROM songplay_table sp

JOIN song_table s

ON sp.song_id = s.song_id

JOIN artist_table a

ON s.artist_id = a.artist_id

GROUP BY sp.song_id, a.name, s.title

ORDER BY COUNT (*) DESC

LIMIT 10;
